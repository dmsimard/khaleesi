---
- name: test host connection
  hosts: all:!localhost
  tasks:
    - name: test ssh
      command: hostname

    - name: check distro
      command: cat /etc/redhat-release

    - name: set fact stack user home
      set_fact: instack_user_home=/home/{{ provisioner.remote_user }}

- name: build or import images
  hosts: undercloud
  tasks:
    - name: setup environment vars
      template: src=templates/build-img-env.j2 dest=~/build-img-env mode=0755

    - name: ensure /tmp/svc-map-services is absent
      file: path=/tmp/svc-map-services state=absent
      sudo: yes
      when: installer.overcloud_images | default('build') == "build"

    - name: Contents of build-img-env
      shell: >
          cat {{ instack_user_home }}/build-img-env

    - name: build all the images
      shell: >
          source {{ instack_user_home }}/build-img-env;
          source {{ instack_user_home }}/stackrc;
          openstack overcloud image build --all > {{ instack_user_home }}/openstack-build-images.log
      when: installer.overcloud_images | default('build') == "build"

    - name: download the pre-built rdo-manager images
      get_url: url="{{ installer.images[product.name][product.full_version][distro.name][distro.version]}}{{ item }}.tar"
                    dest={{ instack_user_home }}
                    force=no
      with_items:
        - deploy-ramdisk-ironic
        - discovery-ramdisk
        - overcloud-full
      when: installer.overcloud_images is defined and installer.overcloud_images == "import"

    - name: untar the overcloud images
      shell: tar -xvf "{{ instack_user_home }}/{{ item }}.tar"
      with_items:
        - deploy-ramdisk-ironic
        - discovery-ramdisk
        - overcloud-full
      when: installer.overcloud_images is defined and installer.overcloud_images == "import"

    - name: download the fedora-user image
      get_url: url="{{ distro.images['fedora']['21'].remote_file_server }}{{ distro.images['fedora']['21'].guest_image_name }}"
                    dest={{ instack_user_home }}/fedora-user.qcow2
                    force=no timeout=60
      when: installer.overcloud_images is defined and installer.overcloud_images == "import"

    - name: prepare for overcloud by loading the images into glance
      shell: >
          source {{ instack_user_home }}/stackrc;
          openstack overcloud image upload

- name: register and discover nodes
  hosts: undercloud
  tasks:
    - name: register bm nodes with openstack cli
      register: register_nodes_result
      retries: 10
      delay: 10
      until: register_nodes_result.rc == 0
      shell: >
          source {{ instack_user_home }}/stackrc;
          openstack baremetal import --json instackenv.json;


    - name: register bm nodes with ironic
      register: register_nodes_result
      retries: 10
      delay: 10
      until: register_nodes_result.rc == 0
      shell: >
          source {{ instack_user_home }}/stackrc;
          openstack baremetal configure boot

    - name: assign kernel and ramdisk to nodes
      shell: >
          source {{ instack_user_home }}/stackrc;
          openstack baremetal configure boot;

    - name: introspect nodes
      shell: >
          source {{ instack_user_home }}/stackrc;
          openstack baremetal introspection bulk start;

    - name: check instrospections status
      register: introspection_result
      retries: 45
      delay: 20
      until: introspection_result.rc == 0
      shell: >
          source {{ instack_user_home }}/stackrc;
          OUTPUT=$(openstack baremetal introspection bulk status)
          TOTAL_NODES=$(echo "$OUTPUT" | grep -E '\w{8}-\w{4}' | wc -l)
          INTROSPECTED_NODES=$(echo "$OUTPUT" | grep -E ' True *\| *None ' | wc -l)
          [ "$TOTAL_NODES" == "$INTROSPECTED_NODES" ]

    - name: show profile
      shell: >
          source {{ instack_user_home }}/stackrc;
          instack-ironic-deployment  --show-profile;

- include: run-overcloud-matching-{{ installer.match_style | default('basic') }}.yml

- name: wait for nova
  hosts: undercloud
  tasks:
    - name: wait until nova becomes aware of first bare metal instance
      register: vcpu_count_single
      retries: 20
      delay: 15
      until: vcpu_count_single.stdout|int > 0
      ignore_errors: true
      shell: >
          source {{ instack_user_home }}/stackrc;
          nova hypervisor-stats | grep ' vcpus ' | head -n1 | awk '{ print $4; }'

- name: Copy over and modify network config template
  hosts: undercloud
  tasks:
    - name: check that network config file exists
      stat: path="{{base_dir}}/khaleesi-settings/hardware_environments/{{hw_env.env_type}}/network_configs/{{ installer.network.isolation }}/{{ installer.network.isolation }}.yml"
      when: installer.network.isolation != 'none'

    - name: copy over template file (baremetal)
      synchronize: >
        src={{base_dir}}/khaleesi-settings/hardware_environments/{{hw_env.env_type}}/network_configs/{{ installer.network.isolation }}/{{ installer.network.isolation }}.yml dest={{ instack_user_home }}/network-environment.yaml
      when: installer.network.isolation != 'none' and installer.env.type != "virthost"

    - name: copy over template file (virt)
      local_action: shell pushd {{ base_dir }}/khaleesi; rsync --delay-updates -F --compress --archive --rsh "ssh -F ssh.config.ansible -S none -o StrictHostKeyChecking=no" {{base_dir}}/khaleesi-settings/hardware_environments/{{hw_env.env_type}}/network_configs/{{ installer.network.isolation }}/{{ installer.network.isolation }}.yml undercloud:{{ instack_user_home }}/network-environment.yaml
      when: installer.network.isolation != 'none' and installer.env.type == "virthost"

    - name: copy over common environment file (baremetal)
      synchronize: >
        src={{base_dir}}/khaleesi-settings/hardware_environments/common/plan-parameter-neutron-bridge.yaml dest={{ instack_user_home }}/plan-parameter-neutron-bridge.yaml
      when: installer.network.isolation != 'none' and installer.env.type != "virthost" and installer.deploy.type == 'plan'

    - name: copy over common environment file (virt)
      local_action: shell pushd {{ base_dir }}/khaleesi; rsync --delay-updates -F --compress --archive --rsh "ssh -F ssh.config.ansible -S none -o StrictHostKeyChecking=no" {{base_dir}}/khaleesi-settings/hardware_environments/common/plan-parameter-neutron-bridge.yaml undercloud:{{ instack_user_home }}/plan-parameter-neutron-bridge.yaml
      when: installer.network.isolation != 'none' and installer.env.type == "virthost" and installer.deploy.type == 'plan'

    - name: copy over nic-configs default directory
      shell: >
        mkdir {{ instack_user_home }}/nic-configs;
        cp /usr/share/openstack-tripleo-heat-templates/network/config/{{ installer.network.isolation | replace('_', '-') }}/*.yaml {{ instack_user_home }}/nic-configs
      when: installer.network.isolation != 'none' and installer.network.isolation != 'default'

    - name: poll for files to exist
      wait_for: path={{ instack_user_home }}/nic-configs/swift-storage.yaml
      when: installer.network.isolation != 'none' and installer.network.isolation != 'default'

    - name: update network-environment.yml file w/ undercloud ip
      lineinfile: "dest={{ instack_user_home }}/network-environment.yaml state=present  line='  EC2MetadataIp: {{ hostvars['undercloud']['ansible_default_ipv4']['address'] }}'"
      when: installer.network.isolation != 'none'

#note this is part of the instructions, not sure if this is a workaround anymore
- name: setup networking on virt for network isolation
  hosts: undercloud:&virthost
  tasks:
    - name: net-iso virt setup vlans
      when: installer.network.isolation == 'single_nic_vlans'
      shell: >
          source {{ instack_user_home }}/stackrc;
          sudo ovs-vsctl add-port br-ctlplane vlan10 tag=10 -- set interface vlan10 type=internal;
          sudo ip l set dev vlan10 up; sudo ip addr add 172.16.23.251/24 dev vlan10;

- name: update neutron values for undercloud
  hosts: undercloud
  tasks:
    - name: update neutron quota to unlimited
      shell: >
          source {{ instack_user_home }}/stackrc;
          neutron quota-update --port -1;

- name: deploy the overcloud
  hosts: undercloud
  tasks:
    - name: get ironic node ids (workaround for bz 1246641)
      shell: >
        source {{ instack_user_home }}/stackrc;
        ironic node-list | grep 'None' | awk '{ print $2; }'
      register: ironic_node_ids
      when: workarounds.enabled is defined and workarounds.enabled|bool

    - name: power off ironic nodes (workaround for bz 1246641)
      shell: >
        source {{ instack_user_home }}/stackrc;
        ironic node-set-power-state {{item}} 'off'
      with_items: ironic_node_ids.stdout_lines
      when: workarounds.enabled is defined and workarounds.enabled|bool

    - name: get number of nodes that could be used for the overcloud
      shell: >
        if [ -f  {{ instack_user_home }}/instackenv.json ]; then
          cat {{ instack_user_home }}/instackenv.json | grep pm_addr |  wc -l
        else
          cat {{ installer.nodes.node_count | default('3') }}
        fi
      register: number_of_possible_nodes

    - name: poll for nodes to be in powered off state
      register: ironic_node_power_off
      shell: >
        source {{ instack_user_home }}/stackrc;
        ironic node-list | grep 'power off' |  wc -l
      until: ironic_node_power_off.stdout == number_of_possible_nodes.stdout

    - name: get plan list
      register: overcloud_uuid_result
      shell: >
          source {{ instack_user_home }}/stackrc;
          openstack management plan list | grep overcloud | cut -d " " -f2

    - name: set fact for openstack management plan
      set_fact:
        overcloud_uuid: "{{ overcloud_uuid_result.stdout }}"

    - name: copy template file with environment variables for overcloud nodes
      template:
        src=templates/deploy-nodes.j2
        dest={{ instack_user_home }}/deploy-nodesrc
        mode=0755

    - name: copy template file with environment variables for overcloud deploy
      template:
        src=templates/deploy-overcloudrc.j2
        dest={{ instack_user_home }}/deploy-overcloudrc
        mode=0755

    - name: set plan values for plan based ceph deployments
      shell: >
          source {{ instack_user_home }}/stackrc;
          source {{ instack_user_home }}/deploy-nodesrc;
          if [ "$CEPHSTORAGESCALE" -gt "0" ]; then
            openstack management plan set {{ overcloud_uuid }} \
              -P Controller-1::CinderEnableIscsiBackend=false \
              -P Controller-1::CinderEnableRbdBackend=true \
              -P Controller-1::GlanceBackend=rbd \
              -P Compute-1::NovaEnableRbdBackend=true;
          fi
      when: installer.deploy.type == 'plan'

    - name: echo deploy command
      register: overcloud_deploy_command
      shell: >
          source {{ instack_user_home }}/stackrc;
          source {{ instack_user_home }}/deploy-nodesrc;
          source {{ instack_user_home }}/deploy-overcloudrc;
          echo $DEPLOY_COMMAND

    - name: deploy-overcloud
      register: overcloud_deployment_result
      ignore_errors: yes
      shell: >
          source {{ instack_user_home }}/stackrc;
          {{ overcloud_deploy_command.stdout }} &> overcloud_deployment_console.log

    - name: echo deploy-overcloud return code
      debug: var=overcloud_deployment_result.rc

    - name: heat stack-list
      ignore_errors: yes
      shell: >
          source {{ instack_user_home }}/stackrc;
          heat stack-list

    - name: overcloud deployment logs
      debug: msg=" Please refer to the undercloud log file for detailed status. The deployment debug logs are stored under /home/stack"

    - name: set fact overcloud_deployment_result
      set_fact:
        overcloud_deployment_result: "{{ overcloud_deployment_result.rc }}"


